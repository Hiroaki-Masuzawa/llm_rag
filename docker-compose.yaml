version: '3.8'

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0:11434     # 追加: 全インターフェースで受け付け
    deploy:
      resources:
        reservations:
          devices:
           - driver: nvidia
             capabilities: [utility, compute, video]  
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "5955:8080"
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434  # ←これはOllama用
      - OPENAI_API_BASE_URL=http://rag_api:8000/v1  # ←RAG APIを使わせる
      - OLLAMA_BASE_URL=http://ollama:11434      # 追加: Ollama に接続するために必須
      - DEFAULT_OPENAI_MODEL=rag-local
    depends_on:
      - ollama
      - rag_api
    extra_hosts:
      - "host.docker.internal:host-gateway"     # 追加：必要時にホストを解決可能にする
    volumes:
      - openwebui_data:/app/backend/data
    restart: unless-stopped

  rag_api:
    build:
      context: ./rag
    container_name: rag_api
    ports:
      - "8000:8000"
    volumes:
      - ./rag:/app
    depends_on:
      - ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

volumes:
  ollama_data:
  openwebui_data:
